{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages\r\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages\r\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from h5py)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn)\r\n"
     ]
    }
   ],
   "source": [
    "#if not already installed\n",
    "!python3 -m pip install h5py sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import rdkit\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import RDConfig\n",
    "from rdkit import rdBase\n",
    "from rdkit.Chem.Draw import IPythonConsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"embeddings/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select A Dataset\n",
    "* Datasets are h5py binary files\n",
    "* Each contains an array with the Transformer embeddings for each molecule\n",
    "* And additional arrays with SMILES strings, binding labels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hiv1_protease_2.hdf5']\n"
     ]
    }
   ],
   "source": [
    "#how many epochs was the transformer trained before generating embeddings?\n",
    "epoch_id = \"2\"\n",
    "\n",
    "assays = os.listdir(embeddings_path)\n",
    "if epoch_id is not None:\n",
    "    #assays = [assay for assay in assays if assay.split(\".\")[0].split(\"_\")[-1] == str(epoch_id)]\n",
    "    assays = [assay for assay in assays if assay.split(\".\")[0].split(\"_\")[-1] == str(epoch_id)]\n",
    "print(assays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/hiv1_protease_2.hdf5\n"
     ]
    }
   ],
   "source": [
    "#Set assay_idx to the index of the desired h5py file in the above list\n",
    "assay_idx = 0\n",
    "assay_path = os.path.join(embeddings_path, assays[assay_idx])\n",
    "print(assay_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "assay = h5py.File(assay_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_result = assay['result'] #numeric assay result\n",
    "labels_binding = assay['binding'] #0 or 1 (\"not binding\" / \"binding\")\n",
    "smiles_enc = assay['smiles'] #smiles strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "binding = labels_binding[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2159"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binding.sum() #how many \"binding\" molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(idxs, labels, ratio=1):\n",
    "    no_bind_idxs = idxs[labels[idxs]==0]\n",
    "    bind_idxs = idxs[labels[idxs]==1]\n",
    "    min_len = min(len(bind_idxs), len(no_bind_idxs)) * ratio\n",
    "    \n",
    "    np.random.shuffle(no_bind_idxs)\n",
    "    np.random.shuffle(bind_idxs)\n",
    "    \n",
    "    no_bind_idxs = no_bind_idxs[:min_len]\n",
    "    bind_idxs = bind_idxs[:min_len]\n",
    "    \n",
    "    idxs = np.concatenate((no_bind_idxs, bind_idxs))\n",
    "    np.random.shuffle(idxs)\n",
    "    \n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4318,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#increase ratio to include more \"non-binding\" samples\n",
    "idxs = undersample(np.arange(len(binding)), binding, ratio=1)\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data(data, idxs):\n",
    "    return [torch.tensor(np.stack([d[idx] for idx in idxs])) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4318, 256, 512]) torch.Size([4318, 256]) torch.Size([4318])\n"
     ]
    }
   ],
   "source": [
    "#create pytorch tensors from the arrays\n",
    "sm, y = reduce_data([smiles_enc, binding], idxs)\n",
    "print(sm.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Tanimoto Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = []\n",
    "found_idxs = []\n",
    "for i in range(sm.shape[0]):\n",
    "    smiles = ''.join([chr(round(sm[i,char].item() * 98) + 32) for char in range(1, sm.shape[1])]).strip(chr(129))\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        assert mol is not None\n",
    "        fps.append(Chem.RDKFingerprint(mol))\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        found_idxs.append(i)\n",
    "    except:\n",
    "        print(\"could not load\")\n",
    "found_idxs = torch.tensor(found_idxs)\n",
    "sm, y = sm[found_idxs], y[found_idxs]\n",
    "#f = torch.tensor(np.stack(bits)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = torch.zeros((sm.shape[0], sm.shape[0]), dtype=torch.float)\n",
    "for i in range(sm.shape[0]):\n",
    "    for j in range(i, sm.shape[0]):\n",
    "        similarity = DataStructs.FingerprintSimilarity(fps[i], fps[j])\n",
    "        similarities[i,j] = similarity\n",
    "        similarities[j,i] = similarity\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10)\n",
    "mean_similarities = []\n",
    "for i, (train_idxs, test_idxs) in enumerate(cv.split(sm, y)):\n",
    "    test_similarities = []\n",
    "    for test_idx in test_idxs:\n",
    "        max_similarity = 0\n",
    "        for train_idx in train_idxs:\n",
    "            max_similarity = max(max_similarity, similarities[test_idx, train_idx])\n",
    "        test_similarities.append(max_similarity)\n",
    "    mean_similarities.append(np.array(test_similarities).mean())\n",
    "print(mean_similarities)\n",
    "print(np.array(mean_similarities).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
